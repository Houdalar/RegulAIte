# RegulAIte

**RegulAIte** is a policy-focused **Q&A assistant** built for businessesâ€”especially HR, compliance, or legal teamsâ€”to get accurate answers from internal documents without sifting through lengthy PDFs.

---
## ğŸ¯ The Business Need

- **Stop policy confusion & reduce support tickets**: Employees can ask specific questionsâ€”e.g., leave entitlements, code of conduct rulesâ€”and get immediate responses based on the official handbook.
- **Improve accuracy and reduce hallucinations**: By retrieving context from trusted documents before querying the LLM, RAG ensures that answers are grounded in real dataâ€”not outdated or incorrect facts.
- **Maintain data control**: Businesses retain full control over the documents usedâ€”meaning nothing leaks into LLM training data or inference pipelines.

---

## ğŸš€ Key Features

- **Easy document upload & indexing**: Upload company handbooks, employee policies, or any text/PDF files via a clean Streamlit UI.  
- **Retrievalâ€‘Augmented Generation**: Uses ChromaDB to find relevant excerpts and include them in the prompt.  
- **Dualâ€‘Model Inference**:  
  - **OpenAI gpt-4o-mini** for fast, reliable cloud inference.  
  - **Qwen3-235B-A22B-Instruct-2507** (via Hugging Face) as an openâ€‘source alternative.  
- **Source citations**: Every answer includes expandable original snippets for verification.  
- **Modular, beginnerâ€‘friendly code**: Clear folder structure (`app/` for logic, `ui/` for UI) and a simple LLM factory.

---

## ğŸ¬ Demo

Here's a quick look at **RegulAIte** in action:

![RegulAIte Demo](demo/demo.gif)

---

## ğŸ“¦ Tech Stack

- **Python 3.10+**  
- **LangChain** for pipeline orchestration  
- **ChromaDB** for vector storage & similarity search  
- **Sentenceâ€‘Transformers** (`all-MiniLM-L6-v2`) for embeddings  
- **OpenAI gpt-4o-mini** via `langchain-openai`  
- **Qwen3-235B-A22B-Instruct-2507** via Hugging Face (`HF_TOKEN`)  
- **Streamlit** for the web interface  
- **dotenv** for environmentâ€‘based API keys  

---

## âš™ï¸ Installation & Quickstart

1. **Clone the repo**  
   ```bash
   git clone https://github.com/yourusername/regulaite.git
   cd regulaite
   ```

2. **Create a virtual environment & install dependencies**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    pip install -r requirements.txt
    ```

3. **Create your .env file in the project root with these entries**
    ```bash
    # For OpenAI inference
    OPENAI_API_KEY=sk-...

    # For Hugging Face Qwen inference
    HF_TOKEN=hf-...
    ```
4. **Run the app**
    ```bash
   streamlit run ui/streamlit_app.py
    ```
5. **Use the UI**
- Upload your PDF/TXT documents under Index Documents.  
- Ask naturalâ€‘language questions in the chat box.
- See answers + source snippets.

---

## âš ï¸ Resource Requirements
- **Qwen via Hugging Face** Running Qwenâ€‘2.5/3 locally in full FP32 precision may require 32â€¯GB+ VRAM .
- If you donâ€™t have that, you can still use OpenAI **gpt-4o-mini** in the cloud.
- **LLaMAâ€‘3** inference via Hugging Face is also possible if you have appropriate permissions.

---

## ğŸ“ Folder Structure
  
    regulaite/
    â”œâ”€â”€ app/                 # Core logic: ingestion, embeddings, QA chain, LLM factory
    â”œâ”€â”€ ui/                  # Streamlit UI
    â”œâ”€â”€ data/                # raw upload directory
    â”œâ”€â”€ vectorstore/         # persisted Chroma embeddings
    â”œâ”€â”€ models/              # local model weights (optional)
    â”œâ”€â”€ .env                 # your API keys
    â”œâ”€â”€ requirements.txt     # Python dependencies
    â””â”€â”€ README.md

---

## ğŸ¤ Contributing
Interested in helping out? Please read our [Contributing Guidelines](CONTRIBUTING.md) to get started.
